{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f99f399",
   "metadata": {},
   "source": [
    "# Mouse Microprotein Atlas Dictionary Builder\n",
    "\n",
    "**Pan-tissue Atlas of Novel Differential microprotein Analysis**\n",
    "\n",
    "> ‚ö†Ô∏è **Demonstration Notebook**: This notebook documents the analytical workflow used to build the PANDA atlas. Original data files are not included due to size constraints. See the README for data availability.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The PANDA atlas integrates multiple evidence types for microprotein discovery:\n",
    "\n",
    "| Evidence Type | Description |\n",
    "|--------------|-------------|\n",
    "| **Ribo-seq** | Ribosome profiling from 4 tissues (muscle, heart, fat, liver) |\n",
    "| **ShortStop** | Machine learning microprotein classification |\n",
    "| **SEER** | Mass spectrometry proteogenomics |\n",
    "| **BLASTP** | Homology to Swiss-Prot and TrEMBL |\n",
    "| **smORF types** | Genomic context (uORF, dORF, intergenic, etc.) |\n",
    "| **Tau scores** | Tissue specificity across 11 tissues |\n",
    "| **DESeq2** | Differential expression from exercise studies |\n",
    "| **DeepLoc** | Subcellular localization predictions |\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you use this code, please cite:\n",
    "- Miller et al. (2025) PANDA: Pan-tissue Atlas of Novel Differential microprotein Analysis\n",
    "- ShortStop: Miller et al. (2025) BMC Methods. PMID: 40756675"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef4e617",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abeae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import gzip\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úÖ All packages imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e998310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FILE PATHS (for reference - not executable without data) ===\n",
    "# These paths show the data organization used in our analysis\n",
    "\n",
    "# Base directory\n",
    "BASE_DIR = \"/path/to/panda/data\"\n",
    "\n",
    "# FASTA sequence files\n",
    "FASTA_FILES = {\n",
    "    \"shortstop\": f\"{BASE_DIR}/reference_databases/rp3_riboseq_shortstop_panda_unique_sequences.fasta\",\n",
    "    \"ribo_orf\": f\"{BASE_DIR}/reference_databases/ribo_seq_from_RiboORF_fasta.fasta\",\n",
    "    \"gencode\": f\"{BASE_DIR}/reference_databases/gencode.vM36.pc_translations.fa.gz\",\n",
    "}\n",
    "\n",
    "# Tissue-specific Ribo-seq count files\n",
    "TISSUE_RIBOSEQ_FILES = {\n",
    "    \"muscle\": f\"{BASE_DIR}/riboseq_counts/muscle/combined_ribo_counts.csv_ENMUSG_sequences.csv\",\n",
    "    \"heart\": f\"{BASE_DIR}/riboseq_counts/heart/combined_ribo_counts.csv_ENMUSG_sequences.csv\",\n",
    "    \"fat\": f\"{BASE_DIR}/riboseq_counts/fat/combined_ribo_counts.csv_ENMUSG_sequences.csv\",\n",
    "    \"liver\": f\"{BASE_DIR}/riboseq_counts/liver/combined_ribo_counts.csv_ENMUSG_sequences.csv\",\n",
    "}\n",
    "\n",
    "# Annotation files\n",
    "BLASTP_RESULTS = f\"{BASE_DIR}/reference_databases/blastp_panda_sequences_vs_uniprot.csv\"\n",
    "SMORF_TYPES = f\"{BASE_DIR}/reference_databases/smorf_types_annotations.csv\"\n",
    "TAU_SCORES = f\"{BASE_DIR}/deseq/tau_scores_mgi_symbol_sequences.csv\"\n",
    "DESEQ_RESULTS = f\"{BASE_DIR}/deseq/filtered_results_padj_less_than_0.05_with_sequence.csv\"\n",
    "DEEPLOC_RESULTS = f\"{BASE_DIR}/intermediate_files/deeploc_signalp6_predictions.csv\"\n",
    "\n",
    "# Filtering parameters\n",
    "MICROPROTEIN_MAX_LENGTH = 150  # amino acids\n",
    "MIN_CPM_PER_KB_THRESHOLD = 5.0  # for SAM evidence filtering\n",
    "\n",
    "print(\"üìÅ File paths defined (demonstration only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3131da07",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Reference Sequences\n",
    "\n",
    "Load FASTA files containing:\n",
    "1. **ShortStop candidate sequences** - Novel microprotein predictions\n",
    "2. **RiboORF sequences** - Ribosome profiling-derived ORFs\n",
    "3. **GENCODE translations** - Canonical protein sequences for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5a66d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasta(fasta_path, is_gzipped=False):\n",
    "    \"\"\"\n",
    "    Load sequences from a FASTA file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    fasta_path : str\n",
    "        Path to FASTA file\n",
    "    is_gzipped : bool\n",
    "        Whether file is gzipped\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : {sequence_id: sequence}\n",
    "    \n",
    "    Example output:\n",
    "    {\n",
    "        'ENSMUST00000001234': 'MKVLWAALLVTFLAGCQA...',\n",
    "        'chr1:12345-12456(+)': 'MKFLIVAALCGHNPT...',\n",
    "        ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    sequences = {}\n",
    "    current_id = None\n",
    "    current_seq = []\n",
    "    \n",
    "    opener = gzip.open if is_gzipped else open\n",
    "    mode = 'rt' if is_gzipped else 'r'\n",
    "    \n",
    "    with opener(fasta_path, mode) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if current_id:\n",
    "                    sequences[current_id] = ''.join(current_seq)\n",
    "                current_id = line[1:].split()[0]  # Take first word after >\n",
    "                current_seq = []\n",
    "            else:\n",
    "                current_seq.append(line)\n",
    "        if current_id:\n",
    "            sequences[current_id] = ''.join(current_seq)\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "# Example usage (not executable without data):\n",
    "# shortstop_fasta = load_fasta(FASTA_FILES[\"shortstop\"])\n",
    "# riboorf_fasta = load_fasta(FASTA_FILES[\"ribo_orf\"])\n",
    "# gencode_fasta = load_fasta(FASTA_FILES[\"gencode\"], is_gzipped=True)\n",
    "\n",
    "print(\"‚úÖ FASTA loading function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2558d9",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Build Sequence Dictionary\n",
    "\n",
    "Create a unified dictionary keyed by **amino acid sequence**.\n",
    "\n",
    "This structure enables:\n",
    "- **Deduplication** of identical sequences from different sources\n",
    "- **Cross-referencing** between databases\n",
    "- **Evidence aggregation** for multi-source validation\n",
    "\n",
    "### Dictionary Structure\n",
    "\n",
    "```python\n",
    "sequence_dict = {\n",
    "    'MKVLWAALLVTFLAG...': {  # amino acid sequence as key\n",
    "        'gene_ids': ['ENSMUST00000001234', 'chr1:12345-12456(+)'],\n",
    "        'sources': ['ShortStop', 'Muscle-RiboORF'],\n",
    "        'shortstop': [{'prediction': 'SAM', 'confidence': 0.95}],\n",
    "        'status': ['Swiss-Prot'],  # or 'TrEMBL' or empty\n",
    "        'proteogenomics': ['SEER'],  # if detected by mass spec\n",
    "        'blastp': [{'uniprot_accession_match': 'P12345', 'evalue': 1e-50}],\n",
    "        'smorf_type': [{'smorf_type': 'uORF', 'gene_body': 'Atf4'}]\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ae6af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Initialize Sequence Dictionary ===\n",
    "\n",
    "sequence_dict = {}\n",
    "\n",
    "def add_to_dict(sequence, gene_id, source):\n",
    "    \"\"\"\n",
    "    Add or update a sequence entry in the dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sequence : str\n",
    "        Amino acid sequence (stop codons will be removed)\n",
    "    gene_id : str\n",
    "        Gene/transcript identifier\n",
    "    source : str\n",
    "        Data source (e.g., 'ShortStop', 'Muscle-RiboORF', 'GENCODE')\n",
    "    \"\"\"\n",
    "    # Clean sequence - remove stop codons\n",
    "    clean_seq = sequence.replace('*', '').replace('.', '')\n",
    "    \n",
    "    if not clean_seq:\n",
    "        return\n",
    "    \n",
    "    # Initialize entry if new sequence\n",
    "    if clean_seq not in sequence_dict:\n",
    "        sequence_dict[clean_seq] = {\n",
    "            'gene_ids': [],\n",
    "            'sources': [],\n",
    "            'shortstop': [],\n",
    "            'status': [],\n",
    "            'proteogenomics': [],\n",
    "            'blastp': [],\n",
    "            'smorf_type': []\n",
    "        }\n",
    "    \n",
    "    # Add gene_id if not already present\n",
    "    if gene_id not in sequence_dict[clean_seq]['gene_ids']:\n",
    "        sequence_dict[clean_seq]['gene_ids'].append(gene_id)\n",
    "    \n",
    "    # Add source if not already present\n",
    "    if source not in sequence_dict[clean_seq]['sources']:\n",
    "        sequence_dict[clean_seq]['sources'].append(source)\n",
    "\n",
    "print(\"‚úÖ Dictionary structure initialized\")\n",
    "\n",
    "# Example usage:\n",
    "# for gene_id, seq in shortstop_fasta.items():\n",
    "#     add_to_dict(seq, gene_id, 'ShortStop')\n",
    "# \n",
    "# for gene_id, seq in riboorf_fasta.items():\n",
    "#     # Determine tissue from gene_id\n",
    "#     for tissue in ['Muscle', 'Heart', 'Fat', 'Liver']:\n",
    "#         if tissue.lower() in gene_id.lower():\n",
    "#             add_to_dict(seq, gene_id, f'{tissue}-RiboORF')\n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6de948d",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Integrate BLASTP Annotations\n",
    "\n",
    "Add homology information from BLASTP searches against UniProt:\n",
    "\n",
    "| Match Type | Prefix | Interpretation |\n",
    "|-----------|--------|----------------|\n",
    "| Swiss-Prot | `sp\\|` | Curated canonical protein |\n",
    "| TrEMBL | `tr\\|` | Computationally predicted |\n",
    "| No match | - | Potentially novel sequence |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efde641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_blastp(blastp_path, sequence_dict, gene_to_sequence):\n",
    "    \"\"\"\n",
    "    Add BLASTP results to sequence dictionary.\n",
    "    \n",
    "    BLASTP was run against UniProt with:\n",
    "        blastp -query microproteins.fasta -db uniprot_sprot_trembl \\\n",
    "               -outfmt '6 qseqid sseqid pident length evalue bitscore' \\\n",
    "               -evalue 1e-5 -max_target_seqs 1\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    blastp_path : str\n",
    "        Path to BLASTP results CSV\n",
    "    sequence_dict : dict\n",
    "        Main sequence dictionary\n",
    "    gene_to_sequence : dict\n",
    "        Mapping of gene_id -> sequence for lookup\n",
    "    \"\"\"\n",
    "    blastp_df = pd.read_csv(blastp_path)\n",
    "    print(f\"BLASTP results loaded: {len(blastp_df):,} entries\")\n",
    "    \n",
    "    matched = 0\n",
    "    for _, row in blastp_df.iterrows():\n",
    "        gene_id = row.get('gene_id') or row.get('query_id')\n",
    "        \n",
    "        if gene_id in gene_to_sequence:\n",
    "            seq = gene_to_sequence[gene_id]\n",
    "            \n",
    "            # Add BLASTP hit details\n",
    "            blastp_entry = {\n",
    "                'uniprot_accession_match': row.get('subject_id'),\n",
    "                'evalue': row.get('evalue'),\n",
    "                'microprotein_percentage_match': row.get('pident'),\n",
    "                'blastp_alignment_length': row.get('length'),\n",
    "                'blastp_bit': row.get('bitscore')\n",
    "            }\n",
    "            sequence_dict[seq]['blastp'].append(blastp_entry)\n",
    "            \n",
    "            # Determine Swiss-Prot vs TrEMBL status\n",
    "            subject_id = str(row.get('subject_id', ''))\n",
    "            if 'sp|' in subject_id:\n",
    "                if 'Swiss-Prot' not in sequence_dict[seq]['status']:\n",
    "                    sequence_dict[seq]['status'].append('Swiss-Prot')\n",
    "            elif 'tr|' in subject_id:\n",
    "                if 'TrEMBL' not in sequence_dict[seq]['status']:\n",
    "                    sequence_dict[seq]['status'].append('TrEMBL')\n",
    "            \n",
    "            matched += 1\n",
    "    \n",
    "    print(f\"‚úÖ BLASTP data added: {matched:,} entries matched\")\n",
    "\n",
    "print(\"‚úÖ BLASTP integration function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265ba309",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Add smORF Type Classifications\n",
    "\n",
    "Classify each ORF by its genomic context:\n",
    "\n",
    "| smORF Type | Location | Description |\n",
    "|------------|----------|-------------|\n",
    "| **uORF** | 5' UTR | Upstream of main CDS |\n",
    "| **dORF** | 3' UTR | Downstream of main CDS |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6935e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_smorf_types(smorf_path, sequence_dict, gene_to_sequence):\n",
    "    \"\"\"\n",
    "    Add smORF type annotations to sequence dictionary.\n",
    "    \n",
    "    smORF types were determined by comparing ORF coordinates to \n",
    "    GENCODE annotations using bedtools intersect.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    smorf_path : str\n",
    "        Path to smORF types CSV\n",
    "    sequence_dict : dict\n",
    "        Main sequence dictionary\n",
    "    gene_to_sequence : dict\n",
    "        Mapping of gene_id -> sequence\n",
    "    \"\"\"\n",
    "    smorf_df = pd.read_csv(smorf_path)\n",
    "    print(f\"smORF types loaded: {len(smorf_df):,} entries\")\n",
    "    \n",
    "    matched = 0\n",
    "    for _, row in smorf_df.iterrows():\n",
    "        gene_id = row.get('gene_id')\n",
    "        if gene_id in gene_to_sequence:\n",
    "            seq = gene_to_sequence[gene_id]\n",
    "            smorf_entry = {\n",
    "                'smorf_type': row.get('smorf_type', 'Unknown'),\n",
    "                'gene_body': row.get('gene_body', 'Unknown')  # Host gene if applicable\n",
    "            }\n",
    "            sequence_dict[seq]['smorf_type'].append(smorf_entry)\n",
    "            matched += 1\n",
    "    \n",
    "    print(f\"‚úÖ smORF types added: {matched:,} entries matched\")\n",
    "\n",
    "print(\"‚úÖ smORF type integration function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c876210",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Integrate Multi-Tissue Ribo-seq Data\n",
    "\n",
    "Add ribosome profiling evidence from 4 tissues:\n",
    "\n",
    "### Normalization Strategy\n",
    "1. **CPM** (Counts Per Million): Cross-sample normalization\n",
    "2. **CPM per kb**: Length normalization for fair comparison across ORF sizes\n",
    "\n",
    "$$\\text{CPM} = \\frac{\\text{raw counts}}{\\text{total library counts}} \\times 10^6$$\n",
    "\n",
    "$$\\text{CPM per kb} = \\frac{\\text{CPM} \\times 1000}{\\text{CDS length (bp)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c593b04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tissue_riboseq_data(tissue_name, file_path, sequence_dict, gene_to_sequence):\n",
    "    \"\"\"\n",
    "    Add tissue-specific Ribo-seq CPM data to sequence dictionary.\n",
    "    \n",
    "    Ribo-seq data was generated using:\n",
    "    1. STAR alignment to mm39 genome\n",
    "    2. RiboORF for ORF calling\n",
    "    3. featureCounts for quantification (see featureCounts_riboseq.sh)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tissue_name : str\n",
    "        Name of tissue (e.g., 'muscle', 'heart')\n",
    "    file_path : str\n",
    "        Path to Ribo-seq counts CSV\n",
    "    sequence_dict : dict\n",
    "        Main sequence dictionary\n",
    "    gene_to_sequence : dict\n",
    "        Gene ID to sequence lookup\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Statistics about the addition\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Processing {tissue_name.title()} Ribo-seq ===\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ö†Ô∏è  File not found: {file_path}\")\n",
    "        return {'status': 'file_not_found', 'total_added': 0}\n",
    "    \n",
    "    tissue_df = pd.read_csv(file_path)\n",
    "    print(f\"Loaded: {len(tissue_df):,} genes\")\n",
    "    \n",
    "    # Identify count columns (exclude Geneid and sequence)\n",
    "    exclude_cols = ['Geneid', 'sequence']\n",
    "    count_cols = [c for c in tissue_df.columns if c not in exclude_cols]\n",
    "    \n",
    "    # CPM normalization\n",
    "    for col in count_cols:\n",
    "        tissue_df[col] = pd.to_numeric(tissue_df[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    total_per_sample = tissue_df[count_cols].sum()\n",
    "    for col in count_cols:\n",
    "        tissue_df[f'{col}_cpm'] = (tissue_df[col] / total_per_sample[col]) * 1_000_000\n",
    "    \n",
    "    # Calculate mean CPM across replicates\n",
    "    cpm_cols = [f'{c}_cpm' for c in count_cols]\n",
    "    tissue_df['mean_cpm'] = tissue_df[cpm_cols].mean(axis=1)\n",
    "    \n",
    "    # Initialize tissue field in dictionary\n",
    "    tissue_field = f'{tissue_name}_riboseq'\n",
    "    for seq in sequence_dict:\n",
    "        if tissue_field not in sequence_dict[seq]:\n",
    "            sequence_dict[seq][tissue_field] = []\n",
    "    \n",
    "    # Add data to dictionary\n",
    "    added = 0\n",
    "    for _, row in tissue_df.iterrows():\n",
    "        gene_id = row['Geneid']\n",
    "        if gene_id in gene_to_sequence:\n",
    "            seq = gene_to_sequence[gene_id]\n",
    "            entry = {\n",
    "                'gene_id': gene_id,\n",
    "                f'{tissue_name}_riboseq_count': row['mean_cpm']\n",
    "            }\n",
    "            sequence_dict[seq][tissue_field].append(entry)\n",
    "            added += 1\n",
    "    \n",
    "    print(f\"‚úÖ Added: {added:,} entries\")\n",
    "    return {'status': 'success', 'total_added': added}\n",
    "\n",
    "print(\"‚úÖ Ribo-seq integration function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1449103d",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Add Tau Scores (Tissue Specificity)\n",
    "\n",
    "The **Tau score** measures tissue specificity:\n",
    "\n",
    "$$\\tau = \\frac{\\sum_{i=1}^{n}(1 - \\hat{x}_i)}{n-1}$$\n",
    "\n",
    "Where $\\hat{x}_i = x_i / \\max(x)$ is the normalized expression in tissue $i$.\n",
    "\n",
    "| Tau Value | Interpretation |\n",
    "|-----------|----------------|\n",
    "| 0.0 | Ubiquitously expressed |\n",
    "| 0.5 | Moderate specificity |\n",
    "| 1.0 | Tissue-specific (single tissue) |\n",
    "\n",
    "The **Tau driver** is the tissue with maximum expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58727abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_tau_scores(tau_path, sequence_dict):\n",
    "    \"\"\"\n",
    "    Add Tau scores and tissue expression to sequence dictionary.\n",
    "    \n",
    "    Tau scores were calculated from bulk RNA-seq across 11 tissues:\n",
    "    Adrenal, BrownAdip, Heart, Hypothal, Kidney, Liver, Lung,\n",
    "    Muscle, Pituitary, epididWAT, inguinWAT\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tau_path : str\n",
    "        Path to Tau scores CSV (must have 'sequence' column)\n",
    "    sequence_dict : dict\n",
    "        Main sequence dictionary\n",
    "    \"\"\"\n",
    "    tau_df = pd.read_csv(tau_path)\n",
    "    print(f\"Tau scores loaded: {len(tau_df):,} entries\")\n",
    "    print(f\"Tau range: {tau_df['Tau'].min():.3f} - {tau_df['Tau'].max():.3f}\")\n",
    "    \n",
    "    # Tissue expression columns\n",
    "    tissue_cols = [\n",
    "        'log2CPM_Adrenal', 'log2CPM_BrownAdip', 'log2CPM_Heart',\n",
    "        'log2CPM_Hypothal', 'log2CPM_Kidney', 'log2CPM_Liver',\n",
    "        'log2CPM_Lung', 'log2CPM_Muscle', 'log2CPM_Pituitary',\n",
    "        'log2CPM_epididWAT', 'log2CPM_inguinWAT'\n",
    "    ]\n",
    "    \n",
    "    matched = 0\n",
    "    for _, row in tau_df.iterrows():\n",
    "        seq = row.get('sequence')\n",
    "        if seq and seq in sequence_dict:\n",
    "            sequence_dict[seq]['tau_score'] = row['Tau']\n",
    "            sequence_dict[seq]['tau_driver'] = row['tau_driver']\n",
    "            \n",
    "            # Add all tissue expression values\n",
    "            for col in tissue_cols:\n",
    "                if col in row:\n",
    "                    sequence_dict[seq][col] = row[col]\n",
    "            matched += 1\n",
    "    \n",
    "    print(f\"‚úÖ Tau scores added: {matched:,} sequences matched\")\n",
    "\n",
    "print(\"‚úÖ Tau score integration function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029799f4",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Save Sequence Dictionary\n",
    "\n",
    "Save the complete dictionary as a pickle file for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20bde4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dictionary(sequence_dict, output_dir):\n",
    "    \"\"\"\n",
    "    Save sequence dictionary with timestamp.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sequence_dict : dict\n",
    "        Complete sequence dictionary\n",
    "    output_dir : str\n",
    "        Directory for output files\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Timestamped version for reproducibility\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    timestamped_path = os.path.join(output_dir, f\"sequence_dictionary_{timestamp}.pkl\")\n",
    "    \n",
    "    with open(timestamped_path, 'wb') as f:\n",
    "        pickle.dump(sequence_dict, f)\n",
    "    print(f\"‚úÖ Saved: {timestamped_path}\")\n",
    "    \n",
    "    # Latest version for easy loading\n",
    "    latest_path = os.path.join(output_dir, \"sequence_dictionary_latest.pkl\")\n",
    "    with open(latest_path, 'wb') as f:\n",
    "        pickle.dump(sequence_dict, f)\n",
    "    print(f\"‚úÖ Saved: {latest_path}\")\n",
    "    \n",
    "    print(f\"\\nüìä Dictionary contains {len(sequence_dict):,} unique sequences\")\n",
    "\n",
    "print(\"‚úÖ Save function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0c5cfe",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Create Microprotein Hits Dictionary\n",
    "\n",
    "Filter to microproteins (‚â§150 aa) and create analysis-ready structure with evidence flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbf2108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hits_dictionary(sequence_dict, max_length=150):\n",
    "    \"\"\"\n",
    "    Create filtered dictionary containing only microproteins with evidence.\n",
    "    \n",
    "    Filtering criteria:\n",
    "    - Length ‚â§ 150 amino acids (or Swiss-Prot annotated)\n",
    "    - Has supporting evidence (Ribo-seq, SAM prediction, SEER, or Swiss-Prot)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sequence_dict : dict\n",
    "        Complete sequence dictionary\n",
    "    max_length : int\n",
    "        Maximum microprotein length (default: 150 aa)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Filtered hits dictionary\n",
    "    \"\"\"\n",
    "    hits_dict = {}\n",
    "    \n",
    "    for sequence, data in sequence_dict.items():\n",
    "        seq_length = len(sequence)\n",
    "        has_swiss_prot = 'Swiss-Prot' in data['status']\n",
    "        \n",
    "        # Filter: microproteins OR Swiss-Prot (any size)\n",
    "        if seq_length > max_length and not has_swiss_prot:\n",
    "            continue\n",
    "        \n",
    "        # Determine protein class\n",
    "        protein_class = \"Microprotein\" if seq_length <= max_length else \"Swiss-Prot Protein\"\n",
    "        \n",
    "        # Determine primary smORF type\n",
    "        if has_swiss_prot:\n",
    "            primary_smorf_type = \"Swiss-Prot-MP\" if seq_length <= 150 else \"Swiss-Prot\"\n",
    "        elif 'TrEMBL' in data['status']:\n",
    "            primary_smorf_type = \"TrEMBL\"\n",
    "        elif data.get('smorf_type'):\n",
    "            types = [e.get('smorf_type', 'Unknown') for e in data['smorf_type']]\n",
    "            primary_smorf_type = Counter(types).most_common(1)[0][0]\n",
    "        else:\n",
    "            primary_smorf_type = \"Unknown\"\n",
    "        \n",
    "        # Evidence flags\n",
    "        has_sam = any('SAM' in e.get('prediction', '') for e in data['shortstop'])\n",
    "        has_riboseq = len(data['sources']) > 0\n",
    "        has_seer = 'SEER' in data['proteogenomics']\n",
    "        has_trembl = 'TrEMBL' in data['status']\n",
    "        \n",
    "        # Store in hits dictionary\n",
    "        hits_dict[sequence] = {\n",
    "            'seq_length': seq_length,\n",
    "            'protein_class': protein_class,\n",
    "            'gene_count': len(data['gene_ids']),\n",
    "            'primary_smorf_type': primary_smorf_type,\n",
    "            'has_sam': has_sam,\n",
    "            'has_riboseq': has_riboseq,\n",
    "            'has_seer': has_seer,\n",
    "            'has_swiss_prot': has_swiss_prot,\n",
    "            'has_trembl': has_trembl,\n",
    "            'gene_ids': data['gene_ids'],\n",
    "            'sources': data['sources'],\n",
    "            'tau_score': data.get('tau_score'),\n",
    "            'tau_driver': data.get('tau_driver')\n",
    "        }\n",
    "    \n",
    "    print(f\"‚úÖ Created hits dictionary: {len(hits_dict):,} microprotein sequences\")\n",
    "    return hits_dict\n",
    "\n",
    "print(\"‚úÖ Hits dictionary function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7c0cc2",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Export to CSV\n",
    "\n",
    "Flatten the nested dictionary to a CSV format for analysis in R or other tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ff197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_csv(hits_dict, output_path):\n",
    "    \"\"\"\n",
    "    Convert hits dictionary to flattened CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    hits_dict : dict\n",
    "        Microprotein hits dictionary\n",
    "    output_path : str\n",
    "        Path for output CSV\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for sequence, data in hits_dict.items():\n",
    "        row = {\n",
    "            'sequence': sequence,\n",
    "            'seq_length': data['seq_length'],\n",
    "            'protein_class': data['protein_class'],\n",
    "            'gene_count': data['gene_count'],\n",
    "            'primary_smorf_type': data['primary_smorf_type'],\n",
    "            'gene_ids': '; '.join(data['gene_ids']),\n",
    "            'has_sam': data['has_sam'],\n",
    "            'has_riboseq': data['has_riboseq'],\n",
    "            'has_seer': data['has_seer'],\n",
    "            'has_swiss_prot': data['has_swiss_prot'],\n",
    "            'has_trembl': data['has_trembl'],\n",
    "            'tau_score': data['tau_score'],\n",
    "            'tau_driver': data['tau_driver']\n",
    "        }\n",
    "        \n",
    "        # Simplified status column\n",
    "        if data['has_swiss_prot']:\n",
    "            row['status'] = 'Swiss-Prot'\n",
    "        elif data['has_trembl']:\n",
    "            row['status'] = 'TrEMBL'\n",
    "        else:\n",
    "            row['status'] = 'Noncanonical'\n",
    "        \n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Saved: {output_path}\")\n",
    "    print(f\"üìä {len(df):,} sequences with {len(df.columns)} columns\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ CSV export function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b5f5d1",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Evidence-Based Filtering\n",
    "\n",
    "Apply quality filters to retain high-confidence microproteins.\n",
    "\n",
    "### Filtering Logic\n",
    "\n",
    "A microprotein is retained if it meets **ANY** of:\n",
    "\n",
    "1. **Swiss-Prot-MP**: Annotated in Swiss-Prot AND ‚â§150 aa\n",
    "2. **SEER**: Detected via proteogenomics (mass spectrometry)\n",
    "3. **Ribo-seq + valid type**: Has Ribo-seq evidence AND is NOT oORF/isoORF\n",
    "4. **SAM + CPM threshold**: ShortStop SAM prediction AND CPM/kb > 5 in any tissue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd4b498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_evidence_filter(df):\n",
    "    \"\"\"\n",
    "    Apply multi-door evidence filtering.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Microprotein hits dataframe\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Filtered dataframe\n",
    "    \"\"\"\n",
    "    print(\"=== Applying Evidence-Based Filtering ===\")\n",
    "    \n",
    "    # Evidence flags\n",
    "    is_swiss = (df['primary_smorf_type'] == 'Swiss-Prot-MP').fillna(False)\n",
    "    has_seer = df['has_seer'].fillna(False)\n",
    "    has_riboseq = df['has_riboseq'].fillna(False)\n",
    "    has_sam = df['has_sam'].fillna(False)\n",
    "    \n",
    "    # Type restriction (exclude oORF, isoORF, Unknown)\n",
    "    invalid_types = ['oORF', 'isoORF', 'Unknown', '']\n",
    "    pass_type_check = ~df['primary_smorf_type'].isin(invalid_types)\n",
    "    \n",
    "    # CPM threshold (if columns exist)\n",
    "    cpm_cols = [c for c in df.columns if 'cpm_per_kb' in c.lower()]\n",
    "    if cpm_cols:\n",
    "        pass_cpm_check = (df[cpm_cols].fillna(0) > 5).any(axis=1)\n",
    "    else:\n",
    "        pass_cpm_check = pd.Series([True] * len(df))\n",
    "    \n",
    "    # Apply multi-door filter\n",
    "    filter_mask = (\n",
    "        is_swiss |  # Door 1: Swiss-Prot-MP\n",
    "        has_seer |  # Door 2: SEER proteogenomics\n",
    "        (has_riboseq & pass_type_check) |  # Door 3: Ribo-seq + valid type\n",
    "        (has_sam & pass_cpm_check)  # Door 4: SAM + CPM threshold\n",
    "    )\n",
    "    \n",
    "    filtered_df = df[filter_mask].copy()\n",
    "    \n",
    "    print(f\"Before filtering: {len(df):,}\")\n",
    "    print(f\"After filtering: {len(filtered_df):,}\")\n",
    "    print(f\"\\nEvidence breakdown:\")\n",
    "    print(f\"  Swiss-Prot-MP: {is_swiss.sum():,}\")\n",
    "    print(f\"  SEER: {has_seer.sum():,}\")\n",
    "    print(f\"  Ribo-seq + valid type: {(has_riboseq & pass_type_check).sum():,}\")\n",
    "    print(f\"  SAM + CPM: {(has_sam & pass_cpm_check).sum():,}\")\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "print(\"‚úÖ Evidence filter function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f13e56",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Generate GTF Tracks\n",
    "\n",
    "Create subset GTF files for genome browser visualization (UCSC, IGV) and Circos plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747da2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gtf_subsets(panda_df, gtf_path, output_dir):\n",
    "    \"\"\"\n",
    "    Generate GTF subset files for different evidence types.\n",
    "    \n",
    "    Creates:\n",
    "    - RiboSeq_evidence.gtf\n",
    "    - Swiss-Prot.gtf\n",
    "    - Mitochondria_predicted.gtf\n",
    "    - Secreted_predicted.gtf\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    panda_df : pd.DataFrame\n",
    "        Filtered PANDA dataframe\n",
    "    gtf_path : str\n",
    "        Path to reference GTF\n",
    "    output_dir : str\n",
    "        Directory for output GTF files\n",
    "    \"\"\"\n",
    "    # Load reference GTF\n",
    "    gtf_df = pd.read_csv(\n",
    "        gtf_path, sep='\\t', header=None, comment='#',\n",
    "        names=['chr', 'source', 'feature', 'start', 'end', 'score', 'strand', 'frame', 'attr']\n",
    "    )\n",
    "    gtf_df['gene_id'] = gtf_df['attr'].str.extract(r'gene_id\\s+\"([^\"]+)\"')\n",
    "    print(f\"Reference GTF loaded: {len(gtf_df):,} entries\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    def get_gene_ids(df):\n",
    "        \"\"\"Extract all gene IDs from semicolon-separated column.\"\"\"\n",
    "        ids = set()\n",
    "        for s in df['gene_ids'].dropna():\n",
    "            for gid in str(s).split('; '):\n",
    "                if gid.strip():\n",
    "                    ids.add(gid.strip())\n",
    "        return ids\n",
    "    \n",
    "    def save_subset(gene_ids, filename, desc):\n",
    "        subset = gtf_df[gtf_df['gene_id'].isin(gene_ids)]\n",
    "        if len(subset) > 0:\n",
    "            output = os.path.join(output_dir, filename)\n",
    "            subset.to_csv(output, sep='\\t', header=False, index=False, quoting=3)\n",
    "            print(f\"‚úÖ {desc}: {len(subset):,} entries -> {filename}\")\n",
    "    \n",
    "    print(\"\\n=== Generating GTF Subsets ===\")\n",
    "    \n",
    "    # Ribo-seq evidence\n",
    "    if 'has_riboseq' in panda_df.columns:\n",
    "        riboseq_df = panda_df[panda_df['has_riboseq'] == True]\n",
    "        save_subset(get_gene_ids(riboseq_df), 'RiboSeq_evidence.gtf', 'Ribo-Seq')\n",
    "    \n",
    "    # Swiss-Prot\n",
    "    if 'primary_smorf_type' in panda_df.columns:\n",
    "        swiss_df = panda_df[panda_df['primary_smorf_type'] == 'Swiss-Prot-MP']\n",
    "        save_subset(get_gene_ids(swiss_df), 'Swiss-Prot.gtf', 'Swiss-Prot')\n",
    "    \n",
    "    # Mitochondria (if DeepLoc data available)\n",
    "    if 'Localizations' in panda_df.columns:\n",
    "        mito_df = panda_df[panda_df['Localizations'].str.contains('Mitochondrion', case=False, na=False)]\n",
    "        save_subset(get_gene_ids(mito_df), 'Mitochondria_predicted.gtf', 'Mitochondria')\n",
    "    \n",
    "    # Secreted\n",
    "    if 'Localizations' in panda_df.columns:\n",
    "        sec_df = panda_df[panda_df['Localizations'].str.contains('Extracellular', case=False, na=False)]\n",
    "        save_subset(get_gene_ids(sec_df), 'Secreted_predicted.gtf', 'Secreted')\n",
    "    \n",
    "    print(f\"\\nüìÅ GTF files saved to: {output_dir}\")\n",
    "\n",
    "print(\"‚úÖ GTF generation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d54eccb",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook documented the complete PANDA atlas building workflow:\n",
    "\n",
    "| Step | Description | Output |\n",
    "|------|-------------|--------|\n",
    "| 1 | Load FASTA sequences | `shortstop_fasta`, `riboorf_fasta`, `gencode_fasta` |\n",
    "| 2 | Build sequence dictionary | `sequence_dict` (nested dict) |\n",
    "| 3 | Integrate BLASTP | Swiss-Prot/TrEMBL status |\n",
    "| 4 | Add smORF types | uORF, dORF, etc. |\n",
    "| 5 | Integrate Ribo-seq | CPM from 4 tissues |\n",
    "| 6 | Add Tau scores | Tissue specificity |\n",
    "| 7 | Save dictionary | `sequence_dictionary_latest.pkl` |\n",
    "| 8 | Create hits dict | Microproteins only |\n",
    "| 9 | Export CSV | `sequence_hits_dictionary.csv` |\n",
    "| 10 | Apply filters | Evidence-based filtering |\n",
    "| 11 | Generate GTFs | Visualization tracks |\n",
    "\n",
    "### Final Outputs\n",
    "\n",
    "- `sequence_dictionary_latest.pkl` - Full nested dictionary\n",
    "- `sequence_hits_dictionary.csv` - Flattened microprotein data  \n",
    "- `panda_hits_deseq.csv` - With DESeq2 integration\n",
    "- `panda_hits_deseq_deeploc.csv` - Complete atlas with localization\n",
    "- `circos_files/*.gtf` - GTF tracks for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6955f4e1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
